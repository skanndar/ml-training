# Phase 0: Data Preparation - COMPLETE âœ…

**Status**: All tasks completed successfully
**Date**: 2025-12-17
**Duration**: ~2 hours

## Overview

Phase 0 implemented a complete streaming data pipeline that avoids downloading all 340k images to disk. Instead, images are downloaded on-the-fly during training with an LRU cache.

## Components Implemented

### 1. Streaming Dataset with LRU Cache âœ…

**File**: [models/streaming_dataset.py](models/streaming_dataset.py)

**Features**:
- `LRUImageCache`: Thread-safe cache with configurable size (50-200GB)
- `ImageDownloader`: Downloads with retry logic, validates image format/size
- `StreamingImageDataset`: PyTorch-compatible dataset that downloads on-demand

**Test Results** (20 sample images):
```
âœ… Basic Loading                  PASS
âœ… Cache Functionality            PASS (60.9% hit rate)
âœ… DataLoader Batching            PASS
âœ… Augmentations                  PASS (random transforms working)
âœ… Error Handling                 PASS
```

### 2. DataLoader Factory âœ…

**File**: [models/dataloader_factory.py](models/dataloader_factory.py)

**Features**:
- Creates train/val DataLoaders from YAML configs
- Applies augmentations (resize, crop, flip, rotation, color jitter)
- Regional filtering support
- Multi-worker data loading

**Usage**:
```python
from models.dataloader_factory import create_dataloaders

train_loader, val_loader, info = create_dataloaders(
    config_path='./config/teacher_global.yaml'
)
```

### 3. Dataset Export and Splits âœ…

**Files**:
- [scripts/export_dataset.py](scripts/export_dataset.py)
- [scripts/split_dataset.py](scripts/split_dataset.py)
- [scripts/create_regional_subset.py](scripts/create_regional_subset.py)

**Execution Results**:

#### Full Dataset Export
```bash
python scripts/export_dataset.py --output-dir ./data
```

**Results**:
- Total plants: 8,587
- Total images: 340,749
- Permissive licenses: 161,791 (47.5%)
- Restrictive licenses: 178,958 (52.5%)
- Output: `data/dataset_raw.jsonl`

#### Train/Val/Test Splits
```bash
python scripts/split_dataset.py --input ./data/dataset_raw.jsonl --output ./data
```

**Results**:
- Duplicate URLs removed: 169,368 (same image used for multiple species)
- Unique images: 171,381
- Train: 136,312 samples (79.5%) - 7,120 classes
- Val: 17,372 samples (10.1%) - 4,147 classes
- Test: 17,697 samples (10.3%) - 3,656 classes
- **No data leakage** between splits âœ…
- Permissive licenses in train: 92.4% âœ…

**Outputs**:
- `data/dataset_train.jsonl`
- `data/dataset_val.jsonl`
- `data/dataset_test.jsonl`
- `data/dataset_splits.jsonl` (combined with split labels)
- `data/split_analysis.json`

#### Regional Subset (EU_SW)
```bash
python scripts/create_regional_subset.py --region EU_SW
```

**Results**:
- Region: EU_SW (Spain, Portugal, France, Italy)
- Total images: 24,666
- Classes: 728
- Avg samples/class: 33.9
- Permissive licenses: 96.0% âœ…
- Output: `data/dataset_eu_sw_train.jsonl`

## Dataset Statistics

### Global Dataset (All Regions)
- **Total unique images**: 171,381
- **Species (classes)**: 7,120 in training set
- **Train/Val/Test**: 80/10/10 split
- **License compliance**: 92.4% permissive in training

### Regional Dataset (EU_SW)
- **Images**: 24,666
- **Species**: 728
- **Average samples per species**: 33.9
- **License compliance**: 96.0% permissive

### Region Distribution (Training Set)
| Region | Images | Percentage |
|--------|--------|------------|
| AMERICAS_NORTH | 49,908 | 36.6% |
| UNKNOWN | 35,990 | 26.4% |
| EU_SW | 24,666 | 18.1% |
| EU_NORTH | 17,757 | 13.0% |
| EU_EAST | 7,991 | 5.9% |

### Source Distribution
| Source | Images | Percentage |
|--------|--------|------------|
| iNaturalist | 125,904 | 92.4% |
| Legacy | 10,398 | 7.6% |
| Perenual | 10 | 0.0% |

## Files Created

### Code
```
ml-training/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ streaming_dataset.py        # LRU cache + streaming loader
â”‚   â””â”€â”€ dataloader_factory.py       # DataLoader factory
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ export_dataset.py           # MongoDB â†’ JSONL export
â”‚   â”œâ”€â”€ split_dataset.py            # Train/val/test splits
â”‚   â”œâ”€â”€ create_regional_subset.py   # Regional filtering
â”‚   â”œâ”€â”€ test_streaming.py           # Streaming dataset tests
â”‚   â””â”€â”€ audit_dataset.py            # Dataset analysis (from earlier)
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ paths.yaml                  # Global paths config
â”‚   â”œâ”€â”€ teacher_global.yaml         # ViT-Base global config
â”‚   â”œâ”€â”€ teacher_regional.yaml       # ViT-Base regional config
â”‚   â””â”€â”€ student.yaml                # MobileNetV2 distillation config
â””â”€â”€ requirements.txt                # Python dependencies
```

### Data
```
ml-training/data/
â”œâ”€â”€ dataset_raw.jsonl               # Full export (340k records)
â”œâ”€â”€ dataset_train.jsonl             # Train split (136k unique images)
â”œâ”€â”€ dataset_val.jsonl               # Val split (17k images)
â”œâ”€â”€ dataset_test.jsonl              # Test split (17k images)
â”œâ”€â”€ dataset_eu_sw_train.jsonl       # Regional subset (24k images)
â”œâ”€â”€ dataset_splits.jsonl            # Combined with split labels
â”œâ”€â”€ split_analysis.json             # Split statistics
â”œâ”€â”€ analysis_eu_sw.json             # Regional analysis
â”œâ”€â”€ class_mapping.json              # Class name â†’ index
â””â”€â”€ export_stats.json               # Export statistics
```

## Key Design Decisions

### 1. Streaming vs Pre-downloading
**Decision**: Implement streaming with LRU cache instead of downloading all images.

**Rationale**:
- Current dataset: 340k images (~100-200GB)
- Full dataset target: 900k images (~300-500GB)
- Streaming approach:
  - Downloads images on-the-fly during training
  - Keeps only recently used images in cache (50-200GB configurable)
  - Evicts least recently used images when cache is full
  - Thread-safe for multi-worker DataLoader

**Tradeoffs**:
- âœ… Saves disk space (no need to store 500GB dataset)
- âœ… Can start training immediately
- âœ… Automatically manages cache size
- âš ï¸ First epoch slower (downloading), subsequent epochs fast (cached)
- âš ï¸ Requires stable internet connection

### 2. URL Deduplication
**Decision**: Deduplicate by `image_url` before splitting.

**Rationale**:
- Same image URL appeared in multiple plant records (169k duplicates found)
- Without deduplication: 25k+ URLs leaked between train/val/test
- After deduplication: Zero leakage âœ…

### 3. License Filtering Strategy
**Decision**: Export both permissive and restrictive, filter at training time.

**Rationale**:
- Development: Can use all 340k images for experimentation
- Production: Use only 161k permissive-licensed images
- Config-based: Set `production_only: true` in YAML
- Current training set: 92.4% permissive (safe for production)

### 4. Regional Teacher Approach
**Decision**: Train separate teacher on EU_SW subset instead of fine-tuning global.

**Rationale**:
- EU_SW subset: 24k images, 728 species
- Allows teacher to specialize on regional flora
- Can start from ImageNet or from global teacher checkpoint
- Config `teacher_regional.yaml` supports both approaches

## Dependencies Installed

```bash
# Core ML frameworks
torch                  # PyTorch (CPU version for now)
torchvision           # Vision models and transforms
tensorflow            # For TF.js export later

# Data processing
pymongo               # MongoDB connection
albumentations        # Advanced augmentations
opencv-python         # Image processing backend
Pillow                # Image loading
requests              # HTTP downloads

# Utilities
pyyaml                # Config file parsing
tqdm                  # Progress bars
python-dotenv         # Environment variables
```

## Validation and Testing

### 1. Streaming Dataset Test
```bash
python scripts/test_streaming.py --sample-size 20
```
- âœ… All 5 tests passed
- âœ… Cache hit rate: 60.9%
- âœ… DataLoader batching working
- âœ… Augmentations randomizing correctly

### 2. Data Leakage Check
```bash
python scripts/split_dataset.py
```
- âœ… No URL overlap between train/val/test
- âœ… Stratified by class (latin_name)
- âœ… ~80/10/10 distribution maintained

### 3. License Compliance Check
- âœ… 92.4% permissive in training set
- âœ… 96.0% permissive in EU_SW regional subset
- âœ… Can filter to 100% permissive for production

## Next Steps: Phase 1 - Teacher Global Training

**Objective**: Train ViT-Base model on full global dataset (136k training images, 7,120 classes).

**Tasks**:
1. Install GPU-enabled PyTorch (`torch` with CUDA)
2. Implement training script (`scripts/train_teacher.py`)
3. Implement evaluation metrics (Top-1, Top-5, ECE)
4. Test on small subset first (1000 samples)
5. Run full training (estimate: 15-20 hours on GPU)

**Command**:
```bash
# Test on small subset
python scripts/train_teacher.py \
  --config ./config/teacher_global.yaml \
  --debug \
  --max-samples 1000

# Full training
python scripts/train_teacher.py \
  --config ./config/teacher_global.yaml \
  --resume ./checkpoints/teacher_global/last_checkpoint.pt  # If interrupted
```

**Expected Outputs**:
- `checkpoints/teacher_global/best_model.pt` (checkpoint with highest val accuracy)
- `checkpoints/teacher_global/last_checkpoint.pt` (latest epoch, for resuming)
- `results/teacher_global_v1/metrics.json` (training curves)
- `results/teacher_global_v1/confusion_matrix.png`
- TensorBoard logs in `checkpoints/teacher_global/logs/`

## Configuration Files Ready

All configuration is in place for the next phases:

### Teacher Global
```yaml
# config/teacher_global.yaml
model:
  name: "vit_base_patch16_224"
  pretrained: true

training:
  learning_rate: 1.0e-5
  batch_size: 64
  epochs: 15

data:
  train_jsonl: "./data/dataset_train.jsonl"
  val_jsonl: "./data/dataset_val.jsonl"
```

### Teacher Regional (EU_SW)
```yaml
# config/teacher_regional.yaml
model:
  name: "vit_base_patch16_224"
  # Can start from global checkpoint:
  # init_from: "./checkpoints/teacher_global/best_model.pt"

training:
  learning_rate: 5.0e-6  # Lower LR for fine-tuning
  batch_size: 32
  epochs: 20

data:
  train_jsonl: "./data/dataset_eu_sw_train.jsonl"
  region_filter: "EU_SW"
```

### Student (MobileNetV2)
```yaml
# config/student.yaml
model:
  name: "mobilenetv2_100"

distillation:
  temperature: 3.0
  alpha: 0.7  # KL divergence weight

  teachers:
    - path: "./results/teacher_global_v1/best_model.pt"
      weight: 0.5
    - path: "./results/teacher_regional_v1/best_model.pt"
      weight: 0.5
```

## Summary

Phase 0 successfully implemented a complete streaming data pipeline with:

- âœ… On-the-fly image downloading with LRU cache (avoids storing 340k-900k images)
- âœ… Thread-safe concurrent downloads with retry logic
- âœ… Image validation (format, size, corruption detection)
- âœ… Augmentation pipeline integration (Albumentations)
- âœ… Train/val/test splits with no data leakage
- âœ… Regional subset creation (EU_SW: 24k images, 728 species)
- âœ… License compliance (92.4% permissive in training)
- âœ… Configuration-driven architecture (YAML configs)
- âœ… All components tested and validated

**Ready to proceed to Phase 1: Teacher Global Training** ğŸš€

---

**Total Dataset Size**:
- Full export: 340,749 records
- Unique images: 171,381
- Training images: 136,312
- Validation images: 17,372
- Test images: 17,697
- EU_SW regional: 24,666

**Estimated Timeline** (updated with current data):
- Phase 0 (Data): âœ… Complete (2 hours)
- Phase 1 (Teacher Global): ~15-20 hours GPU training
- Phase 2 (Teacher Regional): ~8-10 hours GPU training
- Phase 3 (Teacher C - if needed): ~15-20 hours GPU training
- Phase 4 (Student Distillation): ~12-15 hours GPU training
- Phase 5 (Evaluation): ~2-3 hours
- Phase 6 (TF.js Export): ~3-4 hours
- **Total**: ~60-75 hours (2.5-3 days continuous GPU training)

User can now continue importing data to reach the target of 900k images, and when ready, simply re-run Phase 0 export/split scripts on the larger dataset.
