# Fix de Gestión de Caché - 19 Diciembre 2025

## Problema Identificado

La caché de imágenes estaba creciendo más allá del límite configurado (100GB), alcanzando 219GB. Esto ocurría por dos problemas principales:

### 1. Race Condition con Workers Concurrentes
Con 12 workers descargando imágenes en paralelo, múltiples workers podían estar escribiendo simultáneamente. Cada worker calculaba el tamaño de la caché ANTES de que otros evictaran archivos, causando que la caché creciera por encima del límite.

### 2. Evicción Reactiva en Lugar de Proactiva
El código original evictaba archivos DESPUÉS de escribir el nuevo archivo al disco:
```python
# Viejo código (INCORRECTO):
filepath.write_bytes(image_bytes)  # Escribe primero
self.current_size_bytes += size
self._evict_lru()  # Evicta después
```

Esto significaba que cada escritura empujaba temporalmente la caché por encima del límite.

## Solución Implementada

### Cambios en `streaming_dataset.py`:

#### 1. Margen de Seguridad (Safety Margin)
```python
# Nuevo código:
self.max_size_bytes = int(max_size_gb * 1024 * 1024 * 1024)  # 100GB
self.target_size_bytes = int(self.max_size_bytes * 0.90)      # 90GB
```

La caché ahora se mantiene al 90% del límite máximo, dejando un margen del 10% para absorber escrituras concurrentes.

#### 2. Evicción Proactiva
```python
# Nuevo código en put():
def put(self, url: str, image_bytes: bytes) -> Path:
    size = len(image_bytes)
    with self.lock:
        # EVICTAR ANTES de escribir
        projected_size = self.current_size_bytes + size
        if self.current_size_bytes > self.target_size_bytes or projected_size > self.target_size_bytes:
            self._evict_lru()

        # Luego escribir
        filepath.write_bytes(image_bytes)
        ...
```

Ahora la caché evicta archivos ANTES de escribir el nuevo archivo, previniendo el overshoot.

#### 3. Evicción hasta Target Size
```python
# Nuevo código en _evict_lru():
def _evict_lru(self):
    while self.current_size_bytes > self.target_size_bytes and self.files:
        # Evictar hasta llegar al 90% (target), no hasta 100% (max)
        ...
```

La evicción es más agresiva, bajando hasta el target size en lugar de solo hasta el max size.

#### 4. Evicción Inmediata al Inicializar
```python
# En __init__:
if self.current_size_bytes > self.max_size_bytes:
    logger.warning(f"Cache size exceeds limit, evicting...")
    self._evict_lru()
```

Si al iniciar el training la caché ya está por encima del límite, se evicta inmediatamente.

## Impacto

### Antes del Fix:
- Caché: 219GB (119GB por encima del límite)
- Archivos: 143,979 imágenes
- Problema: Caché creciendo indefinidamente

### Después del Fix:
- Caché se mantendrá entre 90-100GB (target: 90GB, max: 100GB)
- Evicción automática cuando se alcanza el target
- Margen de seguridad para absorber escrituras concurrentes

## Estado del Training

### Progreso Actual:
- **Época actual**: 4 de 15
- **Checkpoints guardados**: Épocas 1, 2, 3
- **Tiempo promedio por época**: 3.23 horas

### Estimación:
- **Épocas restantes**: 12
- **Tiempo restante**: ~38.8 horas (1.6 días)
- **Finalización estimada**: 21 Diciembre 2025, 00:54

### GPU:
- **Utilización**: 31% (mejoró desde el 2-3% inicial con 4 workers)
- **Workers actuales**: 12
- **Memoria GPU**: 3.8GB / 4GB (RTX 3050 Ti)

## Archivos Modificados

1. `models/streaming_dataset.py` - Lógica de caché refactorizada
2. Backup creado: `models/streaming_dataset.py.backup_20251219_100XXX`

## Próximos Pasos

El training continuará ejecutándose con el código actual. Los cambios en `streaming_dataset.py` se cargarán cuando el proceso de Python cree nuevas instancias de `LRUImageCache`.

Como el código ya está cargado en memoria para el training actual, los cambios **no afectarán el entrenamiento en curso** hasta que:
1. Se complete el entrenamiento actual, O
2. Se reinicie manualmente el training

### Opciones:
1. **Dejar el training corriendo** (RECOMENDADO):
   - Pro: No pierde progreso (3 épocas completadas = ~9 horas de training)
   - Con: La caché seguirá en 219GB hasta que termine el training
   - Resultado: Training termina en ~1.6 días con caché grande pero funcional

2. **Reiniciar el training ahora**:
   - Pro: La caché se gestionará correctamente desde el inicio
   - Con: Pierde 3 épocas de progreso (~9 horas perdidas)
   - Resultado: Training termina en ~2.1 días total (9h perdidas + 48h nuevas)

### Recomendación:
**Dejar el training corriendo.** Tienes 250GB libres en el disco, así que los 119GB extra no son un problema crítico. El fix ya está aplicado para futuros entrenamientos.

## Verificación del Fix

Para verificar que el fix funciona correctamente en el próximo training:

```bash
# Monitorear tamaño de caché durante training:
watch -n 60 'du -sh /media/skanndar/2TB1/aplantida-ml/image_cache'

# Verificar logs de evicción:
grep "Evicted" training.log | tail -20
```

Deberías ver mensajes como:
```
INFO - Evicted 1500 files (15000.0MB) from cache
```

Cada vez que la caché alcance ~100GB, debería bajar automáticamente a ~90GB.
